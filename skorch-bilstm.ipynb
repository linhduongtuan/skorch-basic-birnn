{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blackcellmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext blackcellmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, autograd\n",
    "from torch.nn import functional as F\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "from nltk import casual_tokenize\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 2\n",
    "MSG_LEN = 20\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 100\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 100000\n",
    "OOV_IDX = 0\n",
    "EOS_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wvs(embeddings_path: str, embedding_dim: int, limit=None):\n",
    "    if limit is not None:\n",
    "        limit = int(limit)\n",
    "    with open(embeddings_path) as infile:\n",
    "        if next(infile).split(\" \") == embeddings_path:\n",
    "            # Skip header for fasttext, don't for glove\n",
    "            infile.seek(0)\n",
    "        return pd.read_csv(\n",
    "            infile,\n",
    "            header=None,\n",
    "            delim_whitespace=True,\n",
    "            names=list(range(embedding_dim)),\n",
    "            quoting=csv.QUOTE_NONE,\n",
    "            nrows=limit,\n",
    "            index_col=0,\n",
    "        )\n",
    "\n",
    "\n",
    "all_embeds = load_wvs(\"embeddings.txt\", EMBED_DIM, VOCAB_SIZE)\n",
    "raw_embeds = np.vstack([np.zeros(EMBED_DIM), all_embeds.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idxs = {tok: idx + 1 for idx, tok in enumerate(all_embeds.index.values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    idxs = [token_idxs.get(tok, OOV_IDX) for tok in text.split(' ')[:MSG_LEN]]\n",
    "    return np.array(idxs + [EOS_IDX] * (MSG_LEN - len(idxs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(topic_num: int):\n",
    "    df = pd.read_csv(\"data/{}.csv\".format(topic_num))\n",
    "    presents = df[df[\"present\"] == 1]\n",
    "    missings = df[df[\"present\"] == 0]\n",
    "    train = pd.concat(\n",
    "        [\n",
    "            presents.iloc[: int(presents.shape[0] // TRAIN_SPLIT)],\n",
    "            missings.iloc[: int(missings.shape[0] // TRAIN_SPLIT)],\n",
    "        ],\n",
    "        axis=\"rows\",\n",
    "    )\n",
    "    validation = pd.concat(\n",
    "        [\n",
    "            presents.iloc[int(presents.shape[0] // TRAIN_SPLIT) :],\n",
    "            missings.iloc[int(missings.shape[0] // TRAIN_SPLIT) :],\n",
    "        ],\n",
    "        axis=\"rows\",\n",
    "    )\n",
    "    return train, validation\n",
    "\n",
    "\n",
    "def make_x_and_y(dataset):\n",
    "    x = [vectorize(text) for text in dataset['text'].values.tolist()]\n",
    "    y = dataset['present'].values\n",
    "    return torch.LongTensor(np.vstack(x).astype(np.int64)), torch.LongTensor(y.astype(np.int64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation = make_dataset(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x, training_y = make_x_and_y(training)\n",
    "validation_x, validation_y = make_x_and_y(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([65, 20]), torch.Size([65]))"
      ]
     },
     "execution_count": 953,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x.shape, training_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prf1(predictions, true):\n",
    "    p = precision_score(predictions, true)\n",
    "    r = recall_score(predictions, true)\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=F.relu):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(VOCAB_SIZE + 1, embedding_dim=EMBED_DIM)\n",
    "        self.word_embeddings.weight = nn.Parameter(torch.FloatTensor(raw_embeds))\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        self.lstm = nn.GRU(\n",
    "            EMBED_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            dropout=0.2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.softmax = nn.Linear(HIDDEN_DIM * 4, 2)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, doc):\n",
    "        embeds = self.word_embeddings(doc)\n",
    "        if self.training:\n",
    "            embeds = self.dropout(embeds.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        tag_space = self.softmax(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    torch.max(self.dropout(lstm_out), 1)[0],\n",
    "                    torch.mean(self.dropout(lstm_out), 1),\n",
    "                ],\n",
    "                1,\n",
    "            )\n",
    "        )\n",
    "        tag_scores = F.softmax(tag_space, dim=-1)\n",
    "        return tag_scores\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (\n",
    "            autograd.Variable(torch.zeros(2, 1, HIDDEN_DIM)),\n",
    "            autograd.Variable(torch.zeros(2, 1, HIDDEN_DIM)),\n",
    "        )\n",
    "\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    Classifier,\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    lr=0.2,\n",
    "    train_split=predefined_split(Dataset(validation_x, validation_y)),\n",
    "    callbacks=[EarlyStopping(patience=5, monitor='valid_loss')]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module!\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5730\u001b[0m       \u001b[32m0.8060\u001b[0m        \u001b[35m0.5387\u001b[0m  0.2695\n",
      "      2        0.7150       0.8060        \u001b[35m0.5054\u001b[0m  0.3071\n",
      "      3        0.6389       0.8060        \u001b[35m0.4933\u001b[0m  0.2895\n",
      "      4        0.6135       0.8060        \u001b[35m0.4855\u001b[0m  0.2912\n",
      "      5        0.5920       0.8060        \u001b[35m0.4804\u001b[0m  0.2722\n",
      "      6        0.5836       0.8060        \u001b[35m0.4785\u001b[0m  0.3068\n",
      "      7        0.5798       0.8060        \u001b[35m0.4729\u001b[0m  0.2890\n",
      "      8        \u001b[36m0.5620\u001b[0m       0.8060        \u001b[35m0.4708\u001b[0m  0.2859\n",
      "      9        \u001b[36m0.5608\u001b[0m       0.8060        \u001b[35m0.4685\u001b[0m  0.2980\n",
      "     10        \u001b[36m0.5502\u001b[0m       0.8060        \u001b[35m0.4651\u001b[0m  0.3004\n",
      "     11        0.5531       0.8060        \u001b[35m0.4622\u001b[0m  0.2811\n",
      "     12        \u001b[36m0.5357\u001b[0m       0.8060        \u001b[35m0.4614\u001b[0m  0.2950\n",
      "     13        0.5405       0.8060        \u001b[35m0.4613\u001b[0m  0.2803\n",
      "     14        0.5429       0.8060        \u001b[35m0.4573\u001b[0m  0.2971\n",
      "     15        \u001b[36m0.5253\u001b[0m       0.8060        \u001b[35m0.4542\u001b[0m  0.2796\n",
      "     16        \u001b[36m0.5238\u001b[0m       0.8060        \u001b[35m0.4515\u001b[0m  0.2940\n",
      "     17        \u001b[36m0.5209\u001b[0m       0.8060        \u001b[35m0.4498\u001b[0m  0.2759\n",
      "     18        \u001b[36m0.5170\u001b[0m       0.8060        \u001b[35m0.4477\u001b[0m  0.2750\n",
      "     19        \u001b[36m0.5064\u001b[0m       0.8060        \u001b[35m0.4451\u001b[0m  0.2727\n",
      "     20        0.5065       0.8060        \u001b[35m0.4422\u001b[0m  0.2795\n",
      "     21        \u001b[36m0.5012\u001b[0m       0.8060        \u001b[35m0.4388\u001b[0m  0.2927\n",
      "     22        \u001b[36m0.4945\u001b[0m       0.8060        \u001b[35m0.4367\u001b[0m  0.2837\n",
      "     23        0.4961       0.8060        \u001b[35m0.4336\u001b[0m  0.3004\n",
      "     24        0.4949       0.8060        \u001b[35m0.4295\u001b[0m  0.2793\n",
      "     25        0.4956       0.8060        \u001b[35m0.4267\u001b[0m  0.2855\n",
      "     26        \u001b[36m0.4675\u001b[0m       0.8060        \u001b[35m0.4239\u001b[0m  0.2991\n",
      "     27        \u001b[36m0.4648\u001b[0m       0.8060        \u001b[35m0.4233\u001b[0m  0.2728\n",
      "     28        0.4755       0.8060        \u001b[35m0.4150\u001b[0m  0.2871\n",
      "     29        \u001b[36m0.4498\u001b[0m       0.8060        \u001b[35m0.4140\u001b[0m  0.2848\n",
      "     30        0.4521       0.8060        \u001b[35m0.4130\u001b[0m  0.3052\n",
      "     31        0.4622       0.8060        \u001b[35m0.4032\u001b[0m  0.2867\n",
      "     32        \u001b[36m0.4235\u001b[0m       0.8060        0.4115  0.2953\n",
      "     33        0.4576       0.8060        \u001b[35m0.3958\u001b[0m  0.2670\n",
      "     34        \u001b[36m0.3926\u001b[0m       0.8060        0.4039  0.2672\n",
      "     35        0.4190       0.8060        \u001b[35m0.3940\u001b[0m  0.2968\n",
      "     36        0.4422       0.8060        \u001b[35m0.3842\u001b[0m  0.2721\n",
      "     37        \u001b[36m0.3453\u001b[0m       0.8060        0.4007  0.2889\n",
      "     38        0.4626       0.8060        \u001b[35m0.3702\u001b[0m  0.2748\n",
      "     39        \u001b[36m0.3159\u001b[0m       0.8060        0.3917  0.2765\n",
      "     40        0.4572       0.8060        \u001b[35m0.3618\u001b[0m  0.3015\n",
      "     41        \u001b[36m0.3124\u001b[0m       0.8060        0.3841  0.2829\n",
      "     42        0.3952       0.8060        \u001b[35m0.3486\u001b[0m  0.2774\n",
      "     43        \u001b[36m0.2764\u001b[0m       0.8060        0.3645  0.2724\n",
      "     44        0.3328       0.8060        0.3876  0.2953\n",
      "     45        0.3183       0.8060        0.3518  0.2887\n",
      "     46        \u001b[36m0.2344\u001b[0m       0.8060        \u001b[35m0.3296\u001b[0m  0.2702\n",
      "     47        0.2966       0.8060        0.4703  0.2853\n",
      "     48        0.5586       0.8060        0.5702  0.2868\n",
      "     49        0.9122       0.8060        0.3369  0.2820\n",
      "     50        \u001b[36m0.2212\u001b[0m       0.8060        \u001b[35m0.3222\u001b[0m  0.2848\n",
      "     51        \u001b[36m0.1683\u001b[0m       0.8060        0.3357  0.3044\n",
      "     52        0.2151       0.8060        0.3511  0.2702\n",
      "     53        0.2273       0.8060        0.3484  0.2823\n",
      "     54        0.1849       \u001b[32m0.8209\u001b[0m        0.3298  0.3011\n",
      "     55        \u001b[36m0.1291\u001b[0m       \u001b[32m0.8955\u001b[0m        \u001b[35m0.2634\u001b[0m  0.2631\n",
      "     56        \u001b[36m0.0974\u001b[0m       0.8955        0.2755  0.2770\n",
      "     57        0.1127       0.8955        0.2731  0.2939\n",
      "     58        \u001b[36m0.0908\u001b[0m       0.8955        0.2823  0.2996\n",
      "     59        \u001b[36m0.0803\u001b[0m       0.8955        0.2639  0.2718\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=Classifier(\n",
       "    (word_embeddings): Embedding(100001, 300)\n",
       "    (lstm): GRU(300, 100, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "    (softmax): Linear(in_features=400, out_features=2, bias=True)\n",
       "    (dropout): Dropout2d(p=0.1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.46153846153846156, 1.0, 0.631578947368421)"
      ]
     },
     "execution_count": 1036,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('validation')\n",
    "prf1(net.predict(validation_x), validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 1037,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('training')\n",
    "prf1(net.predict(training_x), training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(torch.LongTensor(np.vstack([\n",
    "    vectorize('she was not professional'),\n",
    "    vectorize('she was not very professional'),\n",
    "    vectorize('he was not very professional'),\n",
    "    vectorize('he was very rude'),\n",
    "    vectorize('she was very rude'),\n",
    "    vectorize('he was very unprofessional'),\n",
    "    vectorize('they were very condescending'),\n",
    "    vectorize('she was very condescending'),\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.37 s, sys: 39.9 ms, total: 2.41 s\n",
      "Wall time: 608 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "net.predict(torch.cat([validation_x] * 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2070, 20])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([validation_x] * 10).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
